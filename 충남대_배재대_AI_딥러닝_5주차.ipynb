{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "충남대 배재대 AI 딥러닝 5주차",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZPJAUCziFpDPFw2l/YZq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksy-20908/godoheon/blob/main/%EC%B6%A9%EB%82%A8%EB%8C%80_%EB%B0%B0%EC%9E%AC%EB%8C%80_AI_%EB%94%A5%EB%9F%AC%EB%8B%9D_5%EC%A3%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brU9jKopsocw"
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.font_manager as fm\n",
        "import numpy as np\n",
        "\n",
        "print(f'Tensorflow 버전을 확인합니다: {tf.__version__}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuC2611EKhQk"
      },
      "source": [
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aor6pqIlKhu7"
      },
      "source": [
        "print(f'데이터 압축 디렉터리 내부: {os.listdir(dataset_dir)}')\n",
        "\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "print(f'학습 데이터 디렉터리 내부: {os.listdir(train_dir)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft0Z9l4ZLG3E"
      },
      "source": [
        "pos_train_dir = os.path.join(train_dir, 'pos')\n",
        "pos_train_files = os.listdir(pos_train_dir)\n",
        "neg_train_dir = os.path.join(train_dir, 'neg')\n",
        "neg_train_files = os.listdir(neg_train_dir)\n",
        "print('긍정 파일: ', end='')\n",
        "for _ in range(0, 10):\n",
        "    print(random.choice(pos_train_files), end=' ')\n",
        "print()\n",
        "print('부정 파일: ', end='')\n",
        "for _ in range(0, 10):\n",
        "    print(random.choice(neg_train_files), end=' ')\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un3ZeUY_LHHx"
      },
      "source": [
        "sample_file = os.path.join(train_dir, 'pos', '5576_9.txt')\n",
        "with open(sample_file, 'r') as f:\n",
        "    print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xajqDIhSLHKR"
      },
      "source": [
        "print('''학습을 위한 데이터는 이런 구조가 되어야 합니다.\n",
        "train/\n",
        "....pos/\n",
        "........file1.txt\n",
        "........file2.txt\n",
        "....neg/\n",
        "........file1.txt\n",
        "........file2.txt''')\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "print(f'불필요한 데이터 파일을 정리합니다. {remove_dir}')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCUJszYQLVn1"
      },
      "source": [
        "batch_size = 32\n",
        "validation_split = 0.2\n",
        "seed = 20200804\n",
        "\n",
        "print('학습 데이터 세트를 불러옵니다.')\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "        train_dir,\n",
        "        label_mode='binary',\n",
        "        batch_size=batch_size, \n",
        "        validation_split=validation_split, \n",
        "        subset='training',\n",
        "        seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_L9PzEZLV2i"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(3):\n",
        "        print(\"Review\", text_batch.numpy()[i])\n",
        "        print(\"Label\", label_batch.numpy()[i])\n",
        "\n",
        "\n",
        "print('레이블 번호 - 이름 확인!')\n",
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])\n",
        "\n",
        "\n",
        "print('검증 데이터 세트를 불러옵니다.')\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "        train_dir, \n",
        "        label_mode='binary',\n",
        "        batch_size=batch_size,\n",
        "        validation_split=validation_split, \n",
        "        subset='validation', \n",
        "        seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m7Y92r2LHNX"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "    return tf.strings.regex_replace(stripped_html,\n",
        "                                    '[%s]' % re.escape(string.punctuation),\n",
        "                                    '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvpBftW5L0Dj"
      },
      "source": [
        "max_features = 1000\n",
        "sequence_length = 50\n",
        "\n",
        "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        standardize=custom_standardization,\n",
        "        max_tokens=max_features,\n",
        "        output_mode='int',\n",
        "        output_sequence_length=sequence_length)\n",
        "\n",
        "print('테스트 데이터와 레이블을 분리합니다.')\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_e7YmkLL0G-"
      },
      "source": [
        "print('테스트 데이터와 레이블을 분리합니다.')\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)\n",
        "\n",
        "\n",
        "print('학습 완료 후 사용할 테스트 세트를 불러옵니다.')\n",
        "test_dir = os.path.join(dataset_dir, 'test')\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "        test_dir,\n",
        "        label_mode='binary',\n",
        "        batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu9SGZ_tL0Lh"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "print('테스트 데이터를 확인합니다.')\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", raw_train_ds.class_names[int(first_label)])\n",
        "print(\"Vectorized review\", vectorize_text(tf.expand_dims(first_review, -1), first_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q72VWEC7L0Oq"
      },
      "source": [
        "print('각 특징 벡터의 원소 값을 확인할 수 있습니다.')\n",
        "voc_size = len(vectorize_layer.get_vocabulary())\n",
        "print(f'Vocabulary size: {voc_size}')\n",
        "base = 0\n",
        "cnt = 10\n",
        "if voc_size - cnt < base:\n",
        "    print(f'{voc_size-cnt} 보다는 작은 수를 입력해야 합니다.') \n",
        "else:\n",
        "    for i in range(base, base+cnt):\n",
        "        print(f'{i:4d} ---> {vectorize_layer.get_vocabulary()[i]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KXdSIA1L0Jc"
      },
      "source": [
        "print('학습, 검증, 테스트 데이터를 모두 벡터화 합니다.')\n",
        "vec_train_ds = raw_train_ds.map(vectorize_text)\n",
        "vec_val_ds = raw_val_ds.map(vectorize_text)\n",
        "vec_test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "print('데이터 입력부를 최적화합니다.')\n",
        "train_ds = vec_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = vec_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = vec_test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC8U_xv5MjrY"
      },
      "source": [
        "embedding_dim = 4\n",
        "\n",
        "embedding_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(voc_size, embedding_dim),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
        "        tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "embedding_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vntWfAM2MjzY"
      },
      "source": [
        "embedding_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "history = embedding_model.fit(\n",
        "        train_ds,\n",
        "        epochs=10,\n",
        "        validation_data=val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSymK0pGMj2H"
      },
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for i, word in enumerate(vectorize_layer.get_vocabulary()[2:], start=2):\n",
        "    vec = embedding_model.layers[0].get_weights()[0][i] # skip 0, it's padding.\n",
        "    out_m.write(word + \"\\n\")\n",
        "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "except ImportError:\n",
        "    pass\n",
        "else:\n",
        "    files.download('vecs.tsv')\n",
        "    files.download('meta.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TbqQHd7MjxL"
      },
      "source": [
        "print('모델을 정의합니다.')\n",
        "model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Embedding(voc_size, embedding_dim, mask_zero=True),\n",
        "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "                tf.keras.layers.Dense(64, activation='relu'),\n",
        "                tf.keras.layers.Dense(1)])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "print('모델을 준비합니다.')\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
        "              optimizer=tf.keras.optimizers.Adam(), \n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piOyJW6eMjuQ"
      },
      "source": [
        "epochs = 10\n",
        "history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agp52Kj6M-TN"
      },
      "source": [
        "print('모델 성능을 테스트합니다.')\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnd6DQdjM-Wz"
      },
      "source": [
        "history_dict = history.history\n",
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "fig1 = plt.figure(figsize=(6, 10))\n",
        "ax = fig1.add_subplot(2, 1, 1)\n",
        "ax.plot(epochs, loss, 'bo', label='Training loss')\n",
        "ax.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "ax.set_ylim((0, math.ceil(max(max(loss), max(val_loss)))))\n",
        "ax.set_title('Training and validation loss', fontsize=12)\n",
        "ax.set_xlabel('Epochs', fontsize=10)\n",
        "ax.set_ylabel('Loss', fontsize=10)\n",
        "ax.legend()\n",
        "\n",
        "ax = fig1.add_subplot(2, 1, 2)\n",
        "ax.plot(epochs, acc, 'bo', label='Training acc')\n",
        "ax.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "ax.set_ylim((0, math.ceil(max(max(acc), max(val_acc)))))\n",
        "ax.set_title('Training and validation accuracy', fontsize=12)\n",
        "ax.set_xlabel('Epochs', fontsize=10)\n",
        "ax.set_ylabel('Accuracy', fontsize=10)\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXyj2or9NHSI"
      },
      "source": [
        "print('모델 출력')\n",
        "export_model = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    model,\n",
        "    tf.keras.layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGXxwhF4NHqf"
      },
      "source": [
        "user_str = input('테스트 입력: ').strip()\n",
        "predicted = export_model.predict(tf.expand_dims(user_str, -1))[0][0]\n",
        "print(f'예측 결과: {raw_train_ds.class_names[int(round(predicted))]} ({predicted:0.4f})')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUlrDLvycDTS"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Embedding(voc_size, embedding_dim, mask_zero=True),\n",
        "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "                tf.keras.layers.Dense(64, activation='relu'),\n",
        "                tf.keras.layers.Dropout(0.5),\n",
        "                tf.keras.layers.Dense(1)])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}